{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AnimalBreedClassificationFinal.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMmdX5TS7XmeJGiVcvDXepA"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Syt7BR-w5iL3",
        "outputId": "2f16513e-0b29-4ab2-e887-32e972257a02"
      },
      "source": [
        "!wget -O \"animal_breed_classification_ai_challenge-dataset.zip\" \"https://dockship-job-models.s3.ap-south-1.amazonaws.com/6707c47a761bdd2f3c52480c3fd3a6fa?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIDOPTEUZ2LEOQEGQ%2F20210704%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20210704T045943Z&X-Amz-Expires=1800&X-Amz-Signature=81b842e82d0ba25e14c7f9b884bbcb3b6cb4920d8c8a67ed54530bb568a9b1ed&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D%22animal_breed_classification_ai_challenge-dataset.zip%22\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-04 05:02:30--  https://dockship-job-models.s3.ap-south-1.amazonaws.com/6707c47a761bdd2f3c52480c3fd3a6fa?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIDOPTEUZ2LEOQEGQ%2F20210704%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20210704T045943Z&X-Amz-Expires=1800&X-Amz-Signature=81b842e82d0ba25e14c7f9b884bbcb3b6cb4920d8c8a67ed54530bb568a9b1ed&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D%22animal_breed_classification_ai_challenge-dataset.zip%22\n",
            "Resolving dockship-job-models.s3.ap-south-1.amazonaws.com (dockship-job-models.s3.ap-south-1.amazonaws.com)... 52.219.156.10\n",
            "Connecting to dockship-job-models.s3.ap-south-1.amazonaws.com (dockship-job-models.s3.ap-south-1.amazonaws.com)|52.219.156.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 788805172 (752M) [binary/octet-stream]\n",
            "Saving to: ‘animal_breed_classification_ai_challenge-dataset.zip’\n",
            "\n",
            "animal_breed_classi 100%[===================>] 752.26M  12.8MB/s    in 61s     \n",
            "\n",
            "2021-07-04 05:03:32 (12.4 MB/s) - ‘animal_breed_classification_ai_challenge-dataset.zip’ saved [788805172/788805172]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3xyTZ9c_tkB"
      },
      "source": [
        "!unzip 'animal_breed_classification_ai_challenge-dataset.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o38rlSR__1oM"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YO_1Urs_4g-"
      },
      "source": [
        "#'labels' will contain all the breeds of cats and dogs we are required to predict.\n",
        "labels=sorted(os.listdir('/content/TRAIN'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BY2J4P1_61N"
      },
      "source": [
        "#to map each label to its respective class number\n",
        "class_to_num=dict(zip(labels,range(37)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE3gHRqC_9I9",
        "outputId": "ff1abe35-a77f-4e2b-b149-771ead00edeb"
      },
      "source": [
        "#to find the total number of images available for training.\n",
        "count=0\n",
        "for i in labels:\n",
        "  path=os.path.join('/content/TRAIN',i)\n",
        "  for img in os.listdir(path):\n",
        "    count+=1\n",
        "print(count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3xSv4s7__op"
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tWwUPs4ADT8"
      },
      "source": [
        "train_dir='/content/TRAIN'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2pWVoztANgo"
      },
      "source": [
        "#preprocessing the images\n",
        "def image_to_arr(dir,image_size):\n",
        "  X=np.zeros([count,image_size[0],image_size[1],image_size[2]],dtype=np.uint8)\n",
        "  y=np.zeros([count,1],dtype=np.uint8)\n",
        "  j=0 #for indexing X and y\n",
        "  for i in labels:\n",
        "    path=os.path.join('/content/TRAIN',i)\n",
        "    list_of_imgs=sorted(os.listdir(path))\n",
        "    for img in list_of_imgs:\n",
        "      image_pixels=load_img(os.path.join(path,img),target_size=image_size)\n",
        "      X[j]=image_pixels\n",
        "      y[j]=class_to_num[i]\n",
        "      j+=1\n",
        "  y=to_categorical(y)\n",
        "  ind=np.random.permutation(count)\n",
        "  X=X[ind]\n",
        "  y=y[ind]\n",
        "  print('Ouptut Data Size:', X.shape)\n",
        "  print('Ouptut Label Size:', y.shape)\n",
        "  return X, y"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKjkeIrOBe0H"
      },
      "source": [
        "#this image size is compatible for the networks that we wil train our images on.\n",
        "image_size=(299,299,3)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XniEAI1Bgww",
        "outputId": "6bca1999-fe04-47ba-94eb-003c0edf9d6d"
      },
      "source": [
        "train_X,train_y=image_to_arr(train_dir,image_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ouptut Data Size: (5890, 299, 299, 3)\n",
            "Ouptut Label Size: (5890, 37)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7l5CPCBBiod"
      },
      "source": [
        "from keras import Model\n",
        "from keras.layers import BatchNormalization,Dropout,Dense,GlobalAveragePooling2D,Input,InputLayer,Lambda"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe1R9q-6Bqpv"
      },
      "source": [
        "#extracting features for pretrained models.\n",
        "def fetch_features(model_name,data_preprocessor,input_size,data):\n",
        "  #Defining the pipeline\n",
        "  input_layer=Input(input_size)\n",
        "  preprocessor=Lambda(data_preprocessor)(input_layer)\n",
        "  base_model=model_name(weights='imagenet',include_top=False,input_shape=input_size)(preprocessor)\n",
        "  avg=GlobalAveragePooling2D()(base_model)\n",
        "  feature_extractor=Model(inputs=input_layer,outputs=avg)\n",
        "  #extract data\n",
        "  feature_maps=feature_extractor.predict(data,batch_size=32,verbose=1)\n",
        "  print('Feature maps shape: ', feature_maps.shape)\n",
        "  return feature_maps"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utAsdm_3BtJG",
        "outputId": "0f668bcb-fc9e-4bbf-b849-432ec87c8ce6"
      },
      "source": [
        "from keras.applications.xception import Xception,preprocess_input\n",
        "xception_preprocessor=preprocess_input\n",
        "xception_features=fetch_features(Xception,xception_preprocessor,image_size,train_X)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n",
            "83697664/83683744 [==============================] - 1s 0us/step\n",
            "185/185 [==============================] - 87s 242ms/step\n",
            "Feature maps shape:  (5890, 2048)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts1uW-FNsP9d",
        "outputId": "3898a4f3-c256-4c16-b4e4-bb6ff6f60145"
      },
      "source": [
        "from keras.applications.efficientnet import EfficientNetB6,preprocess_input\n",
        "eff_b6_preprocessor=preprocess_input\n",
        "eff_b6_features=fetch_features(EfficientNetB6,eff_b6_preprocessor,image_size,train_X)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb6_notop.h5\n",
            "165240832/165234480 [==============================] - 1s 0us/step\n",
            "165249024/165234480 [==============================] - 1s 0us/step\n",
            "185/185 [==============================] - 104s 489ms/step\n",
            "Feature maps shape:  (5890, 2304)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eY_4UZXv0J1",
        "outputId": "8f396f71-dd2a-4e3b-9cf3-f443eb059c2e"
      },
      "source": [
        "from keras.applications.densenet import DenseNet169,preprocess_input\n",
        "den_169_preprocessor=preprocess_input\n",
        "den_169_features=fetch_features(DenseNet169,den_169_preprocessor,image_size,train_X)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "51879936/51877672 [==============================] - 0s 0us/step\n",
            "51888128/51877672 [==============================] - 0s 0us/step\n",
            "185/185 [==============================] - 48s 199ms/step\n",
            "Feature maps shape:  (5890, 1664)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrHOq8QN6zho",
        "outputId": "3bf49516-09f2-48f5-a224-76486de569c5"
      },
      "source": [
        "from keras.applications.densenet import DenseNet201,preprocess_input\n",
        "den_201_preprocessor=preprocess_input\n",
        "den_201_features=fetch_features(DenseNet201,den_201_preprocessor,image_size,train_X)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74842112/74836368 [==============================] - 1s 0us/step\n",
            "74850304/74836368 [==============================] - 1s 0us/step\n",
            "185/185 [==============================] - 68s 266ms/step\n",
            "Feature maps shape:  (5890, 1920)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsDqha8w5Tp9",
        "outputId": "7c272881-b300-4ba5-ce31-e623060abb79"
      },
      "source": [
        "from keras.applications.densenet import DenseNet121,preprocess_input\n",
        "den_121_preprocessor=preprocess_input\n",
        "den_121_features=fetch_features(DenseNet121,den_121_preprocessor,image_size,train_X)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "29097984/29084464 [==============================] - 0s 0us/step\n",
            "185/185 [==============================] - 38s 160ms/step\n",
            "Feature maps shape:  (5890, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUl0ChCiBDxm"
      },
      "source": [
        "#concatenating all the Dense Net features.\n",
        "den_features=np.concatenate([den_201_features,den_169_features,den_121_features],axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36sJV2MvH8J9"
      },
      "source": [
        "#Concatenating Xception,EfficientNetB6 and DenseNet features for better results.\n",
        "xceptn_eb6_den_features=np.concatenate([xception_features,eff_b6_features,den_features],axis=-1)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bez9FfvMsMgj"
      },
      "source": [
        "#Callbacks\n",
        "from keras.callbacks import EarlyStopping\n",
        "EarlyStop_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "my_callback=[EarlyStop_callback]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HCfQ8ecBzKK"
      },
      "source": [
        "#Building Model\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "model.add(InputLayer(xceptn_eb6_den_features.shape[1:]))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(37,activation='softmax'))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeWSCGtLCM-u"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j68z8SkCUqp",
        "outputId": "5c5e85a7-3b19-4fd9-936b-0f6734c7a806"
      },
      "source": [
        "history = model.fit(xceptn_eb6_den_features,\n",
        "                  train_y,\n",
        "                  batch_size=32,\n",
        "                  epochs=50,\n",
        "                  validation_split=0.1,callbacks=my_callback)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "166/166 [==============================] - 1s 4ms/step - loss: 1.5917 - accuracy: 0.5951 - val_loss: 0.1715 - val_accuracy: 0.9440\n",
            "Epoch 2/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.2164 - accuracy: 0.9354 - val_loss: 0.1595 - val_accuracy: 0.9491\n",
            "Epoch 3/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.1413 - accuracy: 0.9540 - val_loss: 0.1353 - val_accuracy: 0.9559\n",
            "Epoch 4/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.1197 - accuracy: 0.9613 - val_loss: 0.1510 - val_accuracy: 0.9525\n",
            "Epoch 5/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0997 - accuracy: 0.9670 - val_loss: 0.1558 - val_accuracy: 0.9474\n",
            "Epoch 6/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0893 - accuracy: 0.9704 - val_loss: 0.1156 - val_accuracy: 0.9677\n",
            "Epoch 7/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0777 - accuracy: 0.9727 - val_loss: 0.1405 - val_accuracy: 0.9593\n",
            "Epoch 8/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0665 - accuracy: 0.9768 - val_loss: 0.1498 - val_accuracy: 0.9542\n",
            "Epoch 9/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0648 - accuracy: 0.9777 - val_loss: 0.1584 - val_accuracy: 0.9525\n",
            "Epoch 10/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0522 - accuracy: 0.9800 - val_loss: 0.1335 - val_accuracy: 0.9576\n",
            "Epoch 11/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0536 - accuracy: 0.9789 - val_loss: 0.1363 - val_accuracy: 0.9576\n",
            "Epoch 12/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0572 - accuracy: 0.9802 - val_loss: 0.1425 - val_accuracy: 0.9542\n",
            "Epoch 13/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0470 - accuracy: 0.9861 - val_loss: 0.1358 - val_accuracy: 0.9593\n",
            "Epoch 14/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0364 - accuracy: 0.9871 - val_loss: 0.1265 - val_accuracy: 0.9542\n",
            "Epoch 15/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0400 - accuracy: 0.9887 - val_loss: 0.1400 - val_accuracy: 0.9559\n",
            "Epoch 16/50\n",
            "166/166 [==============================] - 1s 3ms/step - loss: 0.0575 - accuracy: 0.9808 - val_loss: 0.1244 - val_accuracy: 0.9660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNHYr3FTBiDj"
      },
      "source": [
        "*Thus from above,we can see we get a validation accuracy of about 96.6% and training accuracy of 98.8%.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhIOi7h8DMSc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}